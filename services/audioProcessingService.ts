
import { VoiceEffect } from "../types";

/**
 * Service for mastering and processing raw audio buffers.
 */

export interface AudioProcessingConfig {
  normalize: boolean;
  trimSilence: boolean;
  maxClipDuration: number; // in seconds, 0 to disable
  effect: VoiceEffect;
  bgMusic?: Blob;
  bgMusicVolume: number;
  speed: number;
  pitch: number;
  generateSubtitles?: boolean;
  generateVideo?: boolean;
  originalText?: string;
}

export interface ProcessedResult {
  audioBlobs: Blob[];
  srtBlob?: Blob;
  videoBlob?: Blob;
}

export const processAudioBuffer = async (
  blob: Blob,
  config: AudioProcessingConfig
): Promise<ProcessedResult> => {
  const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 44100 });
  const arrayBuffer = await blob.arrayBuffer();
  const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

  // Apply Speed and Pitch using OfflineAudioContext
  const offlineCtx = new OfflineAudioContext(
    audioBuffer.numberOfChannels,
    Math.ceil(audioBuffer.length / config.speed),
    audioBuffer.sampleRate
  );

  const source = offlineCtx.createBufferSource();
  source.buffer = audioBuffer;
  
  // Set playback rate for speed
  source.playbackRate.value = config.speed;
  
  // Set detune for pitch (1200 cents per octave)
  if (config.pitch !== 1.0) {
    source.detune.value = Math.log2(config.pitch) * 1200;
  }

  source.connect(offlineCtx.destination);
  source.start(0);
  
  let processedBuffer = await offlineCtx.startRendering();

  // 1. Apply Voice Effects
  if (config.effect !== VoiceEffect.NONE) {
    processedBuffer = applyVoiceEffect(processedBuffer, config.effect);
  }

  // 2. Mix Background Music
  if (config.bgMusic) {
    processedBuffer = await mixBackgroundMusic(processedBuffer, config.bgMusic, config.bgMusicVolume);
  }

  // 3. Trim Silence
  if (config.trimSilence) {
    processedBuffer = trimSilence(processedBuffer);
  }

  // 4. Normalize
  if (config.normalize) {
    processedBuffer = normalizeAudio(processedBuffer);
  }

  // Optional: Subtitles
  let srtBlob: Blob | undefined;
  if (config.generateSubtitles && config.originalText) {
    srtBlob = generateSRT(config.originalText, processedBuffer.duration);
  }

  // Optional: Video (Mocking as a blob that would be generated by a client-side encoder like FFmpeg.wasm)
  let videoBlob: Blob | undefined;
  if (config.generateVideo) {
    videoBlob = await mockVideoGeneration(blob, config.originalText || "Produced Video");
  }

  // 5. Split if necessary
  if (config.maxClipDuration > 0 && processedBuffer.duration > config.maxClipDuration) {
    const chunks = splitAudio(processedBuffer, config.maxClipDuration);
    const audioBlobs = await Promise.all(chunks.map(bufferToWavBlob));
    return { audioBlobs, srtBlob, videoBlob };
  }

  const finalBlob = await bufferToWavBlob(processedBuffer);
  return { audioBlobs: [finalBlob], srtBlob, videoBlob };
};

async function mockVideoGeneration(audioBlob: Blob, text: string): Promise<Blob> {
  // In a real production app, we would use ffmpeg.wasm here to combine a static background with audio.
  // For this demonstration, we return a mock blob representing the video file.
  return new Blob([audioBlob, new TextEncoder().encode(text)], { type: 'video/mp4' });
}

export const stitchAudioBuffers = (buffers: AudioBuffer[]): AudioBuffer => {
  if (buffers.length === 0) throw new Error("No buffers to stitch");
  
  const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
  const totalLength = buffers.reduce((acc, b) => acc + b.length, 0);
  const stitchedBuffer = audioContext.createBuffer(
    buffers[0].numberOfChannels,
    totalLength,
    buffers[0].sampleRate
  );

  let offset = 0;
  buffers.forEach(buffer => {
    for (let channel = 0; channel < buffer.numberOfChannels; channel++) {
      stitchedBuffer.getChannelData(channel).set(buffer.getChannelData(channel), offset);
    }
    offset += buffer.length;
  });

  return stitchedBuffer;
};

export const generateSRT = (text: string, duration: number): Blob => {
  const lines = text.split(/[.!?ред]/).filter(s => s.trim().length > 0);
  const timePerLine = duration / lines.length;
  
  let srtContent = "";
  lines.forEach((line, i) => {
    const start = formatSRTTime(i * timePerLine);
    const end = formatSRTTime((i + 1) * timePerLine);
    srtContent += `${i + 1}\n${start} --> ${end}\n${line.trim()}\n\n`;
  });

  return new Blob([srtContent], { type: "text/plain" });
};

function formatSRTTime(seconds: number): string {
  const date = new Date(0);
  date.setSeconds(seconds);
  const ms = Math.floor((seconds % 1) * 1000);
  return date.toISOString().substr(11, 8) + "," + ms.toString().padStart(3, "0");
}

async function mixBackgroundMusic(voiceBuffer: AudioBuffer, musicBlob: Blob, volume: number): Promise<AudioBuffer> {
  const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
  const musicArrayBuffer = await musicBlob.arrayBuffer();
  const musicBuffer = await audioContext.decodeAudioData(musicArrayBuffer);

  const mixedBuffer = audioContext.createBuffer(
    voiceBuffer.numberOfChannels,
    voiceBuffer.length,
    voiceBuffer.sampleRate
  );

  for (let c = 0; c < voiceBuffer.numberOfChannels; c++) {
    const voiceData = voiceBuffer.getChannelData(c);
    // Use the same channel from music if available, otherwise fallback to mono or loop
    const musicData = musicBuffer.getChannelData(c % musicBuffer.numberOfChannels);
    const mixedData = mixedBuffer.getChannelData(c);

    for (let i = 0; i < voiceData.length; i++) {
      // Loop background music if it's shorter than the voice
      const musicSample = musicData[i % musicBuffer.length] * volume;
      mixedData[i] = voiceData[i] + musicSample;
    }
  }

  return mixedBuffer;
}

export function applyVoiceEffect(buffer: AudioBuffer, effect: VoiceEffect): AudioBuffer {
  const channels = buffer.numberOfChannels;
  const newBuffer = new AudioContext().createBuffer(channels, buffer.length, buffer.sampleRate);

  for (let c = 0; c < channels; c++) {
    const input = buffer.getChannelData(c);
    const output = newBuffer.getChannelData(c);

    if (effect === VoiceEffect.ROBOTIC) {
      for (let i = 0; i < input.length; i++) {
        output[i] = input[i] * Math.sin(i * 0.15);
      }
    } else if (effect === VoiceEffect.ECHO) {
      const delaySamples = Math.floor(buffer.sampleRate * 0.25);
      for (let i = 0; i < input.length; i++) {
        output[i] = input[i] + (i >= delaySamples ? input[i - delaySamples] * 0.5 : 0);
      }
    } else if (effect === VoiceEffect.REVERB) {
      const delaySamples = Math.floor(buffer.sampleRate * 0.05);
      for (let i = 0; i < input.length; i++) {
        output[i] = input[i];
        if (i >= delaySamples) {
          output[i] += output[i - delaySamples] * 0.4;
        }
      }
    } else if (effect === VoiceEffect.WHISPER) {
      for (let i = 0; i < input.length; i++) {
        const noise = (Math.random() * 2 - 1) * 0.2;
        const envelope = Math.abs(input[i]);
        output[i] = (input[i] * 0.3 + noise * envelope);
      }
    } else if (effect === VoiceEffect.NOISE_REDUCTION) {
      const threshold = 0.01;
      for (let i = 0; i < input.length; i++) {
        output[i] = Math.abs(input[i]) < threshold ? 0 : input[i];
      }
    } else {
      output.set(input);
    }
  }
  return newBuffer;
}

function normalizeAudio(buffer: AudioBuffer): AudioBuffer {
  const channels = buffer.numberOfChannels;
  let maxVal = 0;
  for (let c = 0; c < channels; c++) {
    const data = buffer.getChannelData(c);
    for (let i = 0; i < data.length; i++) {
      const abs = Math.abs(data[i]);
      if (abs > maxVal) maxVal = abs;
    }
  }
  if (maxVal === 0) return buffer;
  const scale = 0.98 / maxVal;
  for (let c = 0; c < channels; c++) {
    const data = buffer.getChannelData(c);
    for (let i = 0; i < data.length; i++) {
      data[i] *= scale;
    }
  }
  return buffer;
}

function trimSilence(buffer: AudioBuffer, threshold = 0.005): AudioBuffer {
  const channels = buffer.numberOfChannels;
  const data = buffer.getChannelData(0);
  let start = 0;
  let end = data.length - 1;
  while (start < data.length && Math.abs(data[start]) < threshold) start++;
  while (end > start && Math.abs(data[end]) < threshold) end--;
  const newLength = end - start + 1;
  const trimmedBuffer = new AudioContext().createBuffer(channels, Math.max(1, newLength), buffer.sampleRate);
  for (let c = 0; c < channels; c++) {
    trimmedBuffer.copyToChannel(buffer.getChannelData(c).subarray(start, end + 1), c);
  }
  return trimmedBuffer;
}

function splitAudio(buffer: AudioBuffer, duration: number): AudioBuffer[] {
  const chunks: AudioBuffer[] = [];
  const samplesPerChunk = duration * buffer.sampleRate;
  let offset = 0;
  while (offset < buffer.length) {
    const length = Math.min(samplesPerChunk, buffer.length - offset);
    const chunk = new AudioContext().createBuffer(buffer.numberOfChannels, length, buffer.sampleRate);
    for (let c = 0; c < buffer.numberOfChannels; c++) {
      chunk.copyToChannel(buffer.getChannelData(c).subarray(offset, offset + length), c);
    }
    chunks.push(chunk);
    offset += samplesPerChunk;
  }
  return chunks;
}

export async function bufferToWavBlob(buffer: AudioBuffer): Promise<Blob> {
  const numOfChan = buffer.numberOfChannels;
  const length = buffer.length * numOfChan * 2 + 44;
  const outBuffer = new ArrayBuffer(length);
  const view = new DataView(outBuffer);
  let offset = 0;
  const setUint16 = (data: number) => { view.setUint16(offset, data, true); offset += 2; };
  const setUint32 = (data: number) => { view.setUint32(offset, data, true); offset += 4; };

  setUint32(0x46464952); // "RIFF"
  setUint32(length - 8);
  setUint32(0x45564157); // "WAVE"
  setUint32(0x20746d66); // "fmt "
  setUint32(16);
  setUint16(1);
  setUint16(numOfChan);
  setUint32(buffer.sampleRate);
  setUint32(buffer.sampleRate * 2 * numOfChan);
  setUint16(numOfChan * 2);
  setUint16(16);
  setUint32(0x61746164); // "data"
  setUint32(length - offset - 4);

  for (let i = 0; i < buffer.length; i++) {
    for (let c = 0; c < numOfChan; c++) {
      let sample = Math.max(-1, Math.min(1, buffer.getChannelData(c)[i]));
      sample = (sample < 0 ? sample * 0x8000 : sample * 0x7fff) | 0;
      view.setInt16(offset, sample, true);
      offset += 2;
    }
  }
  return new Blob([outBuffer], { type: "audio/wav" });
}
